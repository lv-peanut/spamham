{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 Spam Ham binary classifier\n",
    "\n",
    "In this notebook I build and evaluate a Naive Bayes classifier\n",
    "\n",
    "The model first uses the SKLearn CountVectorize class to extract each post's word counts into a matrix.\n",
    "\n",
    "And then feeds the matrix to the SKLearn Multinomoal Naive Bayes classifier.\n",
    "\n",
    "* The model was trained on the TRAIN data. \n",
    "* Evaluated on the TEST data trying to optimise precision and recall. I tested a number of variations to optimise these metrics. It was possible to get the accuracy of spam classification fairly high (recall 99% or higher) especially by using ngrams that effectively act as context. But this came at the cost of reducing recall of ham to <50%. In the end I went for a more balanced approach as I was unsure whether it was worse to have false negatives or false positives for spam.\n",
    "* And finally tested on the VALIDATION data\n",
    "\n",
    "Final Results\n",
    "\n",
    "|class | precision | recall | f1-score |\n",
    "|------|-----------|--------|----------|\n",
    "| ham  | 0.77      | 0.74   | 0.76     |\n",
    "| spam | 0.90      | 0.91   | 0.90     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, header=None)\n",
    "df.columns = ['dataset', 'text', 'spamham'] \n",
    "df['target'] = 0\n",
    "df.loc[df.spamham=='spam', 'target'] = 1 # set ham as 0 and spam as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>spamham</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>G.E.M.S Starting a nonprofit organization in t...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Online Shopping w/ AVON Hello Mommas üòòüòò‚ô•Ô∏è If y...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Shop w/ AVON Hello Strong Mommas üòò‚ô•Ô∏è If you lo...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>Shopping If anyone is into make or jewelry or ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Make money from home... http://letty1995.hotsy...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset                                               text spamham  target\n",
       "0   TRAIN  G.E.M.S Starting a nonprofit organization in t...    spam       1\n",
       "1   TRAIN  Online Shopping w/ AVON Hello Mommas üòòüòò‚ô•Ô∏è If y...    spam       1\n",
       "2   TRAIN  Shop w/ AVON Hello Strong Mommas üòò‚ô•Ô∏è If you lo...    spam       1\n",
       "3   TRAIN  Shopping If anyone is into make or jewelry or ...    spam       1\n",
       "4    TEST  Make money from home... http://letty1995.hotsy...    spam       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## split out the train test and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = df.loc[df.dataset=='TRAIN'].text.values\n",
    "test_text = df.loc[df.dataset=='TEST'].text.values\n",
    "validate_text = df.loc[df.dataset=='VALIDATION'].text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.loc[df.dataset=='TRAIN'].target.values\n",
    "y_test = df.loc[df.dataset=='TEST'].target.values\n",
    "y_validate = df.loc[df.dataset=='VALIDATION'].target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,015 documents in the training set\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train_text):,} documents in the training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†intialise nltk functions to test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = word_tokenize # standard tokenizer\n",
    "\n",
    "# tknzr = TweetTokenizer() # tweet friendly tokenizer -- didn't improve the model\n",
    "# tknzr = TweetTokenizer(preserve_case=False, reduce_len=False)\n",
    "# tokenize = tknzr.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count word and document frequencies to filter out common and/or rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = Counter()\n",
    "doc_frequency = Counter()\n",
    "\n",
    "for s in train_text:\n",
    "    \n",
    "    words = tokenize(s.lower())\n",
    "    \n",
    "    word_frequency.update(words)\n",
    "    doc_frequency.update(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    \n",
    "    \"\"\"Wrapper tokenizer function to test impact of different approaches\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    \n",
    "    s : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    tokens : list [str, ]\n",
    "    \"\"\"\n",
    "\n",
    "    tokens =  tokenize(s.lower()) # apply the nltk tokenizer\n",
    "    tokens = [t for t in tokens if doc_frequency[t]>5 and t not in stop_words]# and doc_frequency[t]<3000]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check tokenizer\n",
    "# print(test_text[0])\n",
    "# print()\n",
    "# print(tokenizer(test_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn class to extract and count features/words\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer) #, ngram_range=(1, 3))#, analyzer='word'), tokenizer=tokenizer)\n",
    "\n",
    "# sklean class to learn conditional proabilites\n",
    "\n",
    "# small gain in accuracy from using unifrom priors - not sure why this works\n",
    "classifier = MultinomialNB(fit_prior=False) \n",
    "\n",
    "## tested alternative bayes model that can work better unbalanced samples\n",
    "## no siginficant improvement \n",
    "# classifier = ComplementNB(fit_prior=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(fit_prior=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract sparse matrix with words as columns and documents as rows\n",
    "# each element is the frequency count of the word in document\n",
    "X_train = vectorizer.fit_transform(train_text)\n",
    "\n",
    "## calculate the conditional proabilties for the train data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "I evaluated the model with precision and recall as there is a 3 to 1 inbalance in the spam ham data.\n",
    "\n",
    "I was unsure whether it is worse to allow spam to creep in or to class a user's ham post as spam. So tried to balance the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the model to the test data\n",
    "X_test = vectorizer.transform(test_text)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.78      0.76      0.77       402\n",
      "        spam       0.91      0.92      0.91      1041\n",
      "\n",
      "    accuracy                           0.87      1443\n",
      "   macro avg       0.84      0.84      0.84      1443\n",
      "weighted avg       0.87      0.87      0.87      1443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I played around with a few things to improve the accuracy:\n",
    "    \n",
    "* lower case - **significant improvement**\n",
    "* stemming\n",
    "* nltk twitter specific tokenizer\n",
    "* removing stop words\n",
    "* removing numbers\n",
    "* ngrams\n",
    "* character grams\n",
    "* removing priors - **significant improvement**\n",
    "* removing low frequency words - **significant improvement**\n",
    "* differnt SKLearn naive bayes classifiers\n",
    "\n",
    "<hr>\n",
    "\n",
    "To avoid me having to refer back to wikipedia:\n",
    "* **precision** percentage classed as spam (ham) that are spam (ham)\n",
    "* **recall** percentage of spam (ham) that are classed as spam (ham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the top features/words with the biggest impact\n",
    "\n",
    "The values are the log conditional proability P(word|spam) after laplace smoothing.\n",
    "\n",
    "Larger negative numbers mean lower probability it is spam.\n",
    "\n",
    "\n",
    "Looking at this identified that stop words were high predictors of spam.\n",
    "\n",
    "This may be because spam is more likely to have proper sentences.\n",
    "\n",
    "Although it didn't impact the accuracy much I still decided to remove stop words during tokenization. More work could be done evaluating the high impact features.\n",
    "\n",
    "\n",
    "We can see some words that we would expect to be in spam eg $, looking, free.\n",
    "\n",
    "And you can clearly see something common about the words that suport ham. I suppose these are words that a spammer would not use in case they offended someone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(f'{\"support ham\":30} {\"support spam\":30}')\n",
    "    print()\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(f'{coef_1:10.2f} {fn_1:20} {coef_2:10.2f} {fn_2:20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support ham                    support spam                  \n",
      "\n",
      "    -12.07 abortion                  -3.05 .                   \n",
      "    -12.07 anal                      -3.18 !                   \n",
      "    -12.07 anger                     -3.20 ,                   \n",
      "    -12.07 angry                     -3.94 ‚Äô                   \n",
      "    -12.07 annoyed                   -4.35 :                   \n",
      "    -12.07 bisexual                  -4.45 $                   \n",
      "    -12.07 blocked                   -4.51 ?                   \n",
      "    -12.07 cannabis                  -4.75 )                   \n",
      "    -12.07 cheated                   -4.85 (                   \n",
      "    -12.07 disgusting                -5.03 &                   \n",
      "    -12.07 dumb                      -5.14 https               \n",
      "    -12.07 frustrating               -5.19 get                 \n",
      "    -12.07 fuck                      -5.22 free                \n",
      "    -12.07 fucked                    -5.27 baby                \n",
      "    -12.07 lesbian                   -5.28 home                \n",
      "    -12.07 marijuana                 -5.29 would               \n",
      "    -12.07 miserable                 -5.34 please              \n",
      "    -12.07 opinion                   -5.37 new                 \n",
      "    -12.07 poop                      -5.38 like                \n",
      "    -12.07 porn                      -5.38 looking             \n",
      "    -12.07 pushed                    -5.40 help                \n",
      "    -12.07 smoked                    -5.42 interested          \n",
      "    -12.07 society                   -5.46 need                \n",
      "    -12.07 suicidal                  -5.49 time                \n",
      "    -12.07 threesome                 -5.49 know                \n",
      "    -12.07 tmi                       -5.51 's                  \n",
      "    -12.07 venting                   -5.57 anyone              \n",
      "    -12.07 wtf                       -5.59 work                \n",
      "    -11.38 420                       -5.64 want                \n",
      "    -11.38 acting                    -5.64 make                \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, classifier, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot check text where the model failed\n",
    "\n",
    "One key thing that stood out is that many of the ham posts that are classified as spam appear to have text from a commercial/advert like offering. But the post itself is not spam.\n",
    "\n",
    "\n",
    "I tried cutting post after the first N words. This didn't increase accuracy.\n",
    "\n",
    "\n",
    "Possibly some kind of weighting towards words near the begining would help with this. \n",
    "\n",
    "But if there are too many posts where ham is very similar to spam or spam written like ham then it will push against the boundaries of what an NLP model can distinguish.\n",
    "\n",
    "We could look at the metadata and other information for additional signal, eg past history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Formula samples/coupons I have samples and coupons for similac formula. Does any momma on here need any? I'd be happy to mail it to you! milac FongMoms EWARDS OW SAVE GROW 3 easy steps to FREE Similac! Look inside for savings coupons good toward any Similac product And check your mail for more coming soon. SAVE $5 FREE Similac 1 2 USE COUPONS 3 Use your Similac coupons (We've included some inside) EARN POINTS Earn 5 points for every coupon you use. Points add up and rewards sent automatically GET SIMILAC Reach 35 points and get FREE Similae. Sign up for additional savings and tips at Similac.com/Email-Signup. Enter your membership ID found in the enclosed \"save\" envelope. FRE Sme omes Forem d nd Pet by S grem m e OptiGRO Cm N r r Bdry's Fiest Yer MAX-BASED LPONDE Infant Formula EW Similac HMO For IMMUNE SUPPORT sen ae dnt OptiGRO with Iran Memaciad PRO-ADVANCE NET T 584 NON-GMO Ing essly cep FAbbots Complete Nutrion for Your lainys First Vear ide MLK-SASED POWDER Similac Infant Formula HMO a decae For IMMUNE SUPPORT with Iron NEW! PRO ADVANCE NON Abbott NON-GMO grdu int d optiGRO Aubott Compiete Nutriion for Your Baby's Fiest Yoar Simi MEK-BASED POWDER VTA √©s DHA Similac Infant Formula with Iron HMO PRO-ADVA n –Ω–º–æ For IMMUNE SUPPORT ar UNE PRO-ARVANCE Abbost ur OptiGRo NON-GMO Ingelins nt aialy mnd optiGa Complete Nutriion: POWOER MLBA ive TM for Your Baby's First Vur MILK BASED POWDER –¢HA Similac –ù–º–æ Infant Formula For IMHUNE SUPPORT yy ud na cherte with Iron PRO-ADVANCE NET WT 0.58 02 18.4 g) For more information, visit Similac.com/HMO NON-GMO SAMPLE Similac Infant Formula Similac P0-SENSITIVE a Formila with ko Abott Similac PRO-ADVANCE HMO SUPPORT EWT02(227 \n",
      "------\n",
      "\n",
      "27\n",
      "Maternity photos Hey mamas, I‚Äôm looking for ideas for maternity photos so comment your favorite maternity pictures of yourself or even just a maternity picture you really love. If anyone knows any great maternity photographers in San Antonio, Texas let me know! Has anyone tried take maternity pictures themselves/ with help of a friend how do you feel they turned out?\n",
      "------\n",
      "\n",
      "54\n",
      "Job fair Ur welcome Florida yG 60%4:03 PM JobNewsUSA.com JOB FAIR Wednesday, October 24th, 2018 1 10am-2pm Prime Osborn Con vention Center (Downtown) 1000 Water Street, Jacksonville, FL 32204 Our Sponsor: In Partnership With: CareerSource Que talACKSONVILLE NORTHEAST FLORIDA Companies Attending CSA YECHNICAL AISTITUTE BROOKDALE SColonial Life ONCORDEDeVry University UVAL COUNTY HEALTHCARE GAS GOODWMLHENRY SCHEIN MASSEY ERC FANATICS LEADERQUEST I u d OF NOeHOA STRAYER UFHealth Walgreens Web.com ovation CNDIT SERVICES lendingtree REPUBLIC SERVICES UNIVERSITY UNIVERSITY OF FoRIDA EAL Ally Financial Brookdale Cypress Village CareerSource Northeast Strayer University Student Transportation of America UF Health CNS Healthcare Colonial Life Concorde Career College DeVry University Duval County Public Schools Enhanced Resource Centers Goodwill Industries of North Florida Henry Schein LeaderQuest Massey Services Ovation Credit Services Que' tal Jacksonville Florida CDA Technical Institute City of Jacksonville Military Affairs Walgreens Web.com Fanatics G4S Secure Solutions Republic Services Hundreds of Jobs Available Including: Manager and Supervisor Opportunities, Customer Service Representatives, Collections Associates, Outside and Inside Sales Representatives, Sales Managers, Bilingual Call Center Representatives, Retail Associates, Retail Managers, Pharmacy Tech, School Safety Assistant Teachers, Paraprofessionals, Security Officers, School Bus Drivers, Manager Trainee, Customer Service (Case Advisors), Sales - Credit Analysts, Drivers CDL B, Diesel Mechanics, Operations Supervisors, General Labor/ Route Helper, Heavy Equipment Operators, Licensed Insurance Agents, Career Training Consultant, Technical Instructor, Warehouse Associates, Material Handlers, E-commerce, Maintenance Technicians, CNAS, RNs, LPNS, Housekeeping, Food Service Team Members, Night Shift Seasonal Fan Advocate, Seasonal HR Administrator, Seasonal Janitor, Seasonal Fan Advocate, Payroll Specialist, Jr. File Processing Admin, Risk Mitigation Analyst, Payment Processor Proud to .Support our Troops AVOID THE LINES! Log on today at JobNewsUSA.com to pre-register for this great event. Booth Space still available. Call at (904) 296-3006 for more information. Parking and Admission are FREE Dress Professionally and Bring Plenty of Resumes! Free computer lab for applying to positions online, researching companies and printing resumes. JOBNEWS USA.com Not able to attend this event? Visit www.jobnewsusa.com for information about additional job opportunities and our 2019 recruiting events. \n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    if row.dataset != 'TEST': continue # filter foe test data\n",
    "    \n",
    "    if row.target != 0: continue # filter for spam or ham only. Set as 1 for spam; 0 for ham\n",
    "    \n",
    "    # get the prediction\n",
    "    X_test = vectorizer.transform([row.text])\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    if y_pred[0] == row.target: continue # filter for where we got it wrong\n",
    "\n",
    "    print(i)\n",
    "    print(row.text, end='\\n------\\n\\n') # print post that was misclassified\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    if counter == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the final model on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate = vectorizer.transform(validate_text)\n",
    "y_pred_v = classifier.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76       200\n",
      "           1       0.90      0.91      0.90       486\n",
      "\n",
      "    accuracy                           0.86       686\n",
      "   macro avg       0.83      0.83      0.83       686\n",
      "weighted avg       0.86      0.86      0.86       686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_validate, y_pred_v))#, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models to be used by api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/vectorizer.pkl', 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(vectorizer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/classifier.pkl', 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(classifier, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/doc_frequency_dict.pkl', 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(doc_frequency, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_peanut",
   "language": "python",
   "name": "venv_peanut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
